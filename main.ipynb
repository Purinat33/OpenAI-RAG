{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG using OpenAI LLM\n",
    "\n",
    "Source: \n",
    "1. [Vipra Singh](<https://medium.com/@vipra_singh/building-llm-applications-introduction-part-1-1c90294b155b#4d28>)\n",
    "2. [brightinventions](<https://brightinventions.pl/blog/build-llm-application-with-rag-langchain/>)\n",
    "3. [LangChain RAG](<https://python.langchain.com/v0.2/docs/tutorials/rag/#built-in-chains>)\n",
    "4. [SBERT.net](<sbert.net>)\n",
    "\n",
    "* Only work on 1 single `.txt` file at a time for now\n",
    "* Used a randomly generated story from **ChatGPT**, you could most likely use any: Just change `file_name` to match your new file's name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries:\n",
    "1. `sentence-transformers`: For embedding\n",
    "2. `PyTorch`: For CUDA operation\n",
    "3. `OpenAI`: For LLM\n",
    "4. `Langchain`: For chaining prompts and LLM\n",
    "5. `LlamaIndex`: For indexing\n",
    "6. `FAISS`: For vector storage and retrieval\n",
    "\n",
    "No `LlamaIndex` yet as of this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading environment's variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv_file = dotenv.find_dotenv()\n",
    "dotenv.load_dotenv(dotenv_file)\n",
    "\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "file_name = os.getenv(\"FILE_NAME\")\n",
    "file_url = os.getenv(\"FILE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the source document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Check if file already exists, if not we fetch\n",
    "if not os.path.exists(file_name):\n",
    "    response = requests.get(file_url, stream=True)\n",
    "\n",
    "    with open(file_name, mode='wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=256): # Bytes\n",
    "            file.write(chunk)\n",
    "    print(f\"The file has been downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"File already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the document into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents: \n",
      "file_name: document.txt\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/v0.2/docs/how_to/document_loader_directory/#auto-detect-file-encodings-with-textloader\n",
    "# https://docs.kanaries.net/topics/LangChain/langchain-document-loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(file_path=file_name)\n",
    "document = loader.load()\n",
    "print(f\"Loaded {len(document)} documents: \")\n",
    "for file in document:\n",
    "    print(f\"file_name: {file.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a `Document` object which we can then access the content using `page_content`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and Chunking\n",
    "\n",
    "You may want to split a long document into smaller chunks that can fit into your model's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document has 1 chunk.\n",
      "Document is now splitted into 14 chunks.\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/the-modern-scientist/building-generative-ai-applications-using-langchain-and-openai-apis-ee3212400630\n",
    "# https://python.langchain.com/v0.2/docs/concepts/#text-splitters\n",
    "# https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "texts = document\n",
    "print(f\"Document has {len(texts)} chunk.\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=32,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(document)\n",
    "print(f\"Document is now splitted into {len(texts)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models\n",
    "\n",
    "Embedding models create a vector representation of a piece of text.\n",
    "\n",
    "### Loading embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'sentence-transformers/all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the chunks of list[Document] from chunking steps and getting just the content\n",
    "str_sentences = []\n",
    "for text in texts:\n",
    "    str_sentences.append(text.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding the chunks\n",
    "\n",
    "There are two methods I've seen on the Internet:\n",
    "1. Using `sentence-transformers` directly from SBERT without Langchain integration\n",
    "2. Using `HuggingFaceEmbeddings` from Langchain integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `sentence-transformers` directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SBERT Sentence-transformer\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# embeddings = model.encode(str_sentences)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**:\n",
    "<small>\n",
    "```python\n",
    "[[ 0.00377308  0.00456841  0.04387417 ...  0.09883708  0.02295836\n",
    "  -0.03164019]\n",
    " [ 0.00901005  0.08310206  0.02108724 ...  0.01976032  0.03389375\n",
    "   0.00992528]\n",
    " [-0.00360388  0.02749235  0.16526043 ...  0.12410361 -0.01022669\n",
    "  -0.01867055]\n",
    "```\n",
    " ...\n",
    "```python\n",
    " [-0.00293806  0.02712424  0.07696037 ...  0.10188963  0.05918914\n",
    "   0.01326817]\n",
    " [-0.01563196  0.10205315  0.04504438 ...  0.04045928 -0.05388908\n",
    "  -0.0288553 ]\n",
    " [ 0.01877778 -0.00323905  0.02495503 ...  0.13343076  0.02986323\n",
    "  -0.00972282]]\n",
    "```\n",
    "CPU times: total: 78.1 ms\n",
    "\n",
    "Wall time: 69.8 ms\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `HuggingFaceEmbeddings` Langchain integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.huggingface.HuggingFaceEmbeddings.html\n",
    "# https://python.langchain.com/v0.2/docs/how_to/embed_text/\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = model_id\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Saving/Cachine Embeddings Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/v0.2/docs/how_to/caching_embeddings/\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "store = LocalFileStore('./cache/')\n",
    "\n",
    "cache_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embedding_model, store, namespace=model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no need to call `cache_embedder.embed_documents()` since the Vector Stores (coming up) will handle it internally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the 2 embeddings\n",
    "\n",
    "Despite HuggingFace being a few seconds slower, the value of each embedding is more detailed (more significant digits comparing to `sentence-transformers`) and using Langchain-supported tools all the way to the end might be more convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores\n",
    "\n",
    "We're using `FAISS` for this one, since `ChromaDB` is still brand new there isn't much coverage of it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(texts, cache_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few days later, Thomas realized something unusual. The vegetables near the flower seedlings he had missed were growing better than the others. The flowers attracted bees and butterflies, which helped pollinate his vegetable plants. Thomas started to\n"
     ]
    }
   ],
   "source": [
    "# Similarity search\n",
    "query = \"What happened to Thomas\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few days later, Thomas realized something unusual. The vegetables near the flower seedlings he had missed were growing better than the others. The flowers attracted bees and butterflies, which helped pollinate his vegetable plants. Thomas started to\n"
     ]
    }
   ],
   "source": [
    "# Vector search\n",
    "embedding_vector = cache_embedder.embed_query(query)\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers\n",
    "\n",
    "For taking a query and returning relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few days later, Thomas realized something unusual. The vegetables near the flower seedlings he had missed were growing better than the others. The flowers attracted bees and butterflies, which helped pollinate his vegetable plants. Thomas started to\n"
     ]
    }
   ],
   "source": [
    "# We can use it by:\n",
    "docs = retriever.invoke(input=query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying parameters\n",
    "retriever = db.as_retriever(search_kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few days later, Thomas realized something unusual. The vegetables near the flower seedlings he had missed were growing better than the others. The flowers attracted bees and butterflies, which helped pollinate his vegetable plants. Thomas started to\n",
      "He decided to leave some of the flower seedlings to grow among his vegetables. Over time, both gardens thrived. The flowers attracted more pollinators, and the vegetables grew bigger and healthier. Lily and Thomas learned to appreciate each other's\n",
      "Lily noticed what Thomas had done and felt sad. “My flowers just wanted to spread their beauty,” she thought. But she didn’t say anything to Thomas.\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(input=query)\n",
    "for sentence in docs:\n",
    "    print(sentence.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt and LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Logging for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.re_phraser').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an English teacher teaching elementary students context clues, reading comprehension, and critical thinking. You have the students read from the context text. Your task is to answer questions by either: 1. Directly copy and pasting passages from the context, 2. Infer an answer that might not directly be contained in the context, 3. Use critical thinking.\n",
    "The context is as follow: <context>{context}</context>\n",
    "The question asked by a student is as follow:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/v0.2/docs/integrations/retrievers/re_phrase/\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html\n",
    "\n",
    "import openai\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "retrieval_qa_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", prompt_template),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/v0.2/docs/tutorials/rag/#built-in-chains\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "combined_docs_chain = create_stuff_documents_chain(\n",
    "    llm, retrieval_qa_chat_prompt\n",
    ")\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, combined_docs_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"What is Thomas' problem with Lily?\",\n",
       " 'context': [Document(metadata={'source': 'document.txt'}, page_content='Lily noticed what Thomas had done and felt sad. “My flowers just wanted to spread their beauty,” she thought. But she didn’t say anything to Thomas.'),\n",
       "  Document(metadata={'source': 'document.txt'}, page_content=\"He decided to leave some of the flower seedlings to grow among his vegetables. Over time, both gardens thrived. The flowers attracted more pollinators, and the vegetables grew bigger and healthier. Lily and Thomas learned to appreciate each other's\"),\n",
       "  Document(metadata={'source': 'document.txt'}, page_content='From then on, Lily and Thomas’s gardens became the talk of the village. People admired the beautiful mix of flowers and vegetables and enjoyed the produce they shared.')],\n",
       " 'answer': \"Thomas' problem with Lily is that he initially didn't appreciate the beauty of her flowers and decided to remove them from his garden. Lily noticed this and felt sad, but she didn't say anything to Thomas.\"}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running\n",
    "retrieval_chain.invoke({'input': \"What is Thomas' problem with Lily?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the context, it can be inferred that Thomas likely had a change of heart and decided to let the flowers grow alongside his vegetables to continue benefiting from the bees and butterflies that were helping pollinate his plants. This change in perspective may have led to a more harmonious garden where both flowers and vegetables thrived together.\n"
     ]
    }
   ],
   "source": [
    "query = \"What do you think happen immediately after the end of the story?\"\n",
    "response = retrieval_chain.invoke({'input': query})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Future Possibility\n",
    "\n",
    "Right now, our RAG isn't able to complicately infer or come up with something new from the existing story.\n",
    "\n",
    "Possible TODO:\n",
    "1. History\n",
    "2. Chatbot\n",
    "3. Streamlit\n",
    "4. Better embeddings and retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{thakur-2020-AugSBERT,\n",
    "  title = \"Augmented {SBERT}: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks\",\n",
    "  author = \"Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes  and Gurevych, Iryna\",\n",
    "  booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n",
    "  month = jun,\n",
    "  year = \"2021\",\n",
    "  address = \"Online\",\n",
    "  publisher = \"Association for Computational Linguistics\",\n",
    "  url = \"https://www.aclweb.org/anthology/2021.naacl-main.28\",\n",
    "  pages = \"296--310\",\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
